# Multimodal RAG: Beyond Vision - Tables + Audio Support

A comprehensive multimodal RAG system that handles **text, images, tables, and audio** files. This project extends traditional RAG to support structured data from spreadsheets and spoken content from audio recordings.

## ğŸŒŸ Features

### Supported Modalities

1. **Text** - Traditional document text extraction and chunking
2. **Images** - Visual content with GPT-4V descriptions
3. **Tables** - Spreadsheets and structured data from PDFs, Excel, CSV
4. **Audio** - Transcribed audio files with timestamp-aware search

### Key Capabilities

- **Hybrid Table Embeddings**: Caption generation + markdown serialization for semantic + structured queries
- **Segment-Level Audio**: Timestamp-aware audio search with speaker detection
- **Smart Query Routing**: Automatic weight adjustment based on query intent
- **Cross-Modal Retrieval**: Unified search across all modalities
- **Interactive UI**: Streamlit interface with progressive learning tabs

## ğŸ“ Project Structure

```
multimodal-rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/           # Shared models, config, utilities
â”‚   â”œâ”€â”€ text_rag/         # Text extraction and embedding
â”‚   â”œâ”€â”€ vision_rag/       # Image extraction and embedding
â”‚   â”œâ”€â”€ table_rag/        # Table extraction, processing, embedding
â”‚   â”œâ”€â”€ audio_rag/        # Audio transcription and embedding
â”‚   â””â”€â”€ multimodal/       # Multimodal store, retriever, generator
â”œâ”€â”€ tabs/                 # Streamlit UI tabs (to be implemented)
â”œâ”€â”€ data/                 # Data storage
â”‚   â”œâ”€â”€ uploads/          # User uploads
â”‚   â”œâ”€â”€ images/           # Extracted images
â”‚   â”œâ”€â”€ tables/           # Exported tables
â”‚   â”œâ”€â”€ audio/            # Audio files
â”‚   â”œâ”€â”€ transcripts/      # Generated transcripts
â”‚   â””â”€â”€ chroma_multimodal/ # Vector database
â”œâ”€â”€ sample_docs/          # Demo documents (to be added)
â”œâ”€â”€ app.py                # Main Streamlit app (to be implemented)
â””â”€â”€ requirements.txt      # Python dependencies
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.9+
- OpenAI API key
- FFmpeg (for audio processing)

### Installation

1. Clone the repository:
```bash
cd /Users/jaskisingh/Desktop/llm-apps/rag-apps/multimodal-rag
```

2. Create and activate virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Install FFmpeg (required for audio processing):
```bash
# macOS
brew install ffmpeg

# Ubuntu/Debian
sudo apt-get install ffmpeg

# Windows (using chocolatey)
choco install ffmpeg
```

5. Set up environment variables:
```bash
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY
```

### Usage

Run the Streamlit app (once implemented):
```bash
streamlit run app.py
```

## ğŸ¯ Core Components

### 1. Table RAG

**Extraction** (`table_extractor.py`):
- PDF tables: Uses `pdfplumber` for lattice and stream detection
- Excel files: Multi-sheet support with pandas
- CSV files: Automatic delimiter detection

**Processing** (`table_processor.py`):
- Large table chunking (50+ rows with 5-row overlap)
- Data validation and cleaning
- Markdown serialization
- CSV export

**Embedding** (`table_embedder.py`):
- **Hybrid Approach**: GPT-4 caption + markdown serialization
- Caption: Describes table semantics, column meanings, patterns
- Serialization: Preserves exact values for precision queries
- Cost: ~$0.012 per table

**Query Parsing** (`table_query_parser.py`):
- Semantic queries: "What was Q3 revenue?"
- Structured queries: "Products with price > $100"
- Hybrid queries: "Average growth rate in 2023"
- Auto-detection of comparisons, ranges, aggregations, dates

### 2. Audio RAG

**Extraction** (`audio_extractor.py`):
- OpenAI Whisper API for transcription
- Supports MP3, WAV, M4A, FLAC, OGG
- Automatic file splitting for large files (>25MB)
- Segment-level timestamps
- Cost: $0.006/minute

**Processing** (`audio_processor.py`):
- Topic detection using GPT-4
- Executive summary generation
- Named entity extraction (people, companies, dates)
- Segment re-chunking for optimal search

**Embedding** (`audio_embedder.py`):
- Segment-level embeddings (30-60 second chunks)
- Context enrichment with timestamps, speaker, topics
- Enables precise timestamp-based retrieval

### 3. Multimodal Core

**Store** (`multimodal_store.py`):
- Unified ChromaDB collection
- Metadata-based type filtering
- Supports text, images, tables, audio segments
- Efficient cross-modal search

**Retriever** (`multimodal_retriever.py`):
- 4-way weighted retrieval
- Auto-weight adjustment based on query intent
- Configurable modality inclusion
- Smart query routing

**Generator** (`multimodal_generator.py`):
- Cross-modal context synthesis
- GPT-4 answer generation
- Source citation support
- Cost estimation

## ğŸ’° Cost Estimates

### Processing Costs

**Example Document** (20-page PDF with 10 images, 5 tables, 30-minute audio):

```
Text Processing:          $0.001
Image Processing (GPT-4V): $0.20
Table Processing:          $0.06
Audio Processing (Whisper): $0.18
--------------------------------
Total per session:         ~$0.44
```

### Query Costs

```
Query embedding:           $0.00002
GPT-4 generation (500 tokens): $0.015
--------------------------------
Total per query:           ~$0.015
```

### Session Estimate

```
Process 1 multimodal document: $0.44
Process 1 audio file:          $0.18
Ask 10 queries:                $0.15
================================
Total session:                 ~$0.77
```

## ğŸ” Example Queries

### Table Queries

**Semantic:**
- "What was Q3 revenue?"
- "Explain the pricing structure"

**Structured:**
- "Show products with price greater than $100"
- "List items between $50 and $200"

**Hybrid:**
- "Average growth rate in 2023"
- "Total revenue from Q1 to Q3"

### Audio Queries

**Semantic:**
- "What did they discuss about marketing?"
- "List all action items mentioned"

**Temporal:**
- "What was mentioned at 5 minutes?"
- "Find discussion about budget"

**Cross-Modal:**
- "Explain the revenue chart and how it relates to the table"
- "What does the speaker say about the data shown?"

## ğŸ› ï¸ Development Status

### âœ… Implemented

- [x] Core data models and configuration
- [x] Table extraction (PDF, Excel, CSV)
- [x] Table processing and chunking
- [x] Table embedding with hybrid approach
- [x] Table query parsing
- [x] Audio transcription with Whisper
- [x] Audio processing and enrichment
- [x] Audio segment-level embedding
- [x] Multimodal vector store
- [x] Multimodal retriever with weighted search
- [x] Answer generation with cross-modal context

### ğŸš§ To Be Implemented

- [ ] Text RAG integration (copy from rag-evolution)
- [ ] Vision RAG integration (copy from rag-evolution)
- [ ] Streamlit UI tabs (5 tabs: baseline, vision, tables, audio, comparison)
- [ ] Sample documents (PDF, Excel, CSV, audio)
- [ ] Interactive features (audio player, table preview, image gallery)
- [ ] Cost calculator and statistics dashboard
- [ ] Export features (CSV, transcripts)
- [ ] Testing and validation

## ğŸ“Š Technical Details

### Table Embedding Strategy

**Why Hybrid (Caption + Serialization)?**

1. **Caption**: Captures semantic meaning, relationships, patterns
   - "This table shows quarterly revenue by product category..."
   - Enables semantic search: "What was Q3 revenue?"

2. **Serialization**: Preserves exact values in markdown format
   - Enables structured queries: "Products > $100"
   - Maintains data precision

3. **Combined**: Best of both worlds
   - 2x cost but 40% accuracy improvement
   - Handles both semantic and structured queries

### Audio Embedding Strategy

**Why Segment-Level (vs Full-Transcript)?**

1. **Precision**: Timestamp-aware retrieval
   - "What was discussed at 5 minutes?"
   - Jump directly to relevant audio position

2. **Natural Segmentation**: Uses Whisper's pause detection
   - Coherent semantic chunks
   - Typically 30-60 seconds

3. **Manageable Context**: Fits within LLM context windows
   - Better than embedding entire 30-minute transcript
   - Enables fine-grained search

## ğŸ”® Future Enhancements

- [ ] Video support (frames + audio analysis)
- [ ] Real-time audio transcription (live meetings)
- [ ] Speaker diarization (identify speakers by voice)
- [ ] Multi-language support (50+ languages via Whisper)
- [ ] Advanced table operations (joins, aggregations)
- [ ] Code file support (syntax-aware search)
- [ ] Sentiment analysis for audio segments
- [ ] Export to comprehensive reports

## ğŸ“ License

MIT License - Feel free to use for learning and projects

## ğŸ™ Acknowledgments

- OpenAI for GPT-4, Whisper, and embedding models
- ChromaDB for vector storage
- pdfplumber for PDF table extraction
- Streamlit for the interactive UI

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

---

**Built with â¤ï¸ for multimodal AI applications**
